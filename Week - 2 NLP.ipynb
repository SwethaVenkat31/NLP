{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week - 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 NLP - Preprocessing Tokenization , Stemming, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['run', 'runner', 'run', 'in', 'race']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "text=\"Running runners run in race\"\n",
    "tokens=word_tokenize(text)\n",
    "stemmers=PorterStemmer()\n",
    "stemmed=[stemmers.stem(token)for token in tokens]\n",
    "print(\"Stemmed words:\",stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Advanced Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['The', 'striped', 'bat', 'be', 'hang', 'on', 'their', 'foot']\n"
     ]
    }
   ],
   "source": [
    "text='The striped bats are hanging on their feet'\n",
    "tokens=word_tokenize(text)\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "lemmatized=[lemmatizer.lemmatize(w,get_wordnet_pos(t))for w,t, in pos_tag(tokens)]\n",
    "print(\"Lemmatized Words:\", lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Porter's Stemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It simplifies words by reducing them to their root forms, a process known as \"stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['running', 'jumps', 'happily', 'running', 'happily']\n",
      "Stemmed words: ['run', 'jump', 'happili', 'run', 'happili']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create a Porter Stemmer instance\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Example words for stemming\n",
    "words = [\"running\", \"jumps\", \"happily\", \"running\", \"happily\"]\n",
    "\n",
    "# Apply stemming to each word\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Snowball Stemmer, compared to the Porter Stemmer, is multi-lingual as it can handle non-English words. It supports various languages and is based on the 'Snowball' programming language, known for efficient processing of small strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['running', 'jumped', 'happily', 'quickly', 'foxes']\n",
      "Stemmed words: ['run', 'jump', 'happili', 'quick', 'fox']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Choose a language for stemming, for example, English\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# Example words to stem\n",
    "words_to_stem = ['running', 'jumped', 'happily', 'quickly', 'foxes']\n",
    "\n",
    "# Apply Snowball Stemmer\n",
    "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original words:\", words_to_stem)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule-based, very aggressive iterative suffix stripping; repeatedly applies short rules to strip suffixes aggressively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['running', 'jumped', 'happily', 'quickly', 'foxes']\n",
      "Stemmed words: ['run', 'jump', 'happy', 'quick', 'fox']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Create a Lancaster Stemmer instance\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Example words to stem\n",
    "words_to_stem = ['running', 'jumped', 'happily', 'quickly', 'foxes']\n",
    "\n",
    "# Apply Lancaster Stemmer\n",
    "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original words:\", words_to_stem)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Regexp Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uses custom regular expressions to strip suffixes or prefixes; depends entirely on regex patterns user defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: running\n",
      "Stemmed Word: runn\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Create a Regexp Stemmer with a custom rule\n",
    "custom_rule = r'ing$'\n",
    "regexp_stemmer = RegexpStemmer(custom_rule)\n",
    "\n",
    "# Apply the stemmer to a word\n",
    "word = 'running'\n",
    "stemmed_word = regexp_stemmer.stem(word)\n",
    "\n",
    "print(f'Original Word: {word}')\n",
    "print(f'Stemmed Word: {stemmed_word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lovins Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runn\n",
      "fox\n",
      "jump\n",
      "happi\n",
      "quick\n"
     ]
    }
   ],
   "source": [
    "def lovins_stem(word):\n",
    "    suffixes = ['ization', 'tion', 'ing', 'ly', 'ed', 'es', 's']\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Remove the longest suffix found at the end of the word\n",
    "    for suffix in sorted(suffixes, key=len, reverse=True):\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "# Examples\n",
    "print(lovins_stem(\"running\"))      # Output: runn\n",
    "print(lovins_stem(\"foxes\"))    # Output: happin\n",
    "print(lovins_stem(\"jumped\"))  # Output: conditional (no suffix removed)\n",
    "print(lovins_stem(\"happily\")) # Output: organ\n",
    "print(lovins_stem(\"quickly\"))         # Output: cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. N- gram Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'striped'), ('striped', 'bats'), ('bats', 'are'), ('are', 'hanging'), ('hanging', 'on'), ('on', 'their'), ('their', 'feet')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "n = 2  # for bigrams, change to any n you want\n",
    "result = list(ngrams(tokens, n))\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
